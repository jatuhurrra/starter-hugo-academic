---
# Display name
title: Atuhurra Jesse

# Is this the primary user of the site?
superuser: true

# Role/position/tagline
#role: PhD Information Science and Engineering
role: Ph.D. Candidate, Natural Language Processing (NLP), <a href="https://scholar.google.com/citations?view_op=list_works&hl=en&hl=en&user=2Li9kqwAAAAJ">Google Scholar</a></h3>

# Organizations/Affiliations to show in About widget
organizations:
- name: Nara Institute of Science and Technology (NAIST)
  url: http://www.naist.jp/en/

# Short bio (displayed in user profile at end of posts)
bio: My research interests broadly include NLP, Social Robotics, and Representation Learning.

# Interests to show in About widget
interests:
- Natural Language Processing
- Multimodal Foundation Models
- Social Robotics 
- Humanâ€”Robot Interaction
- Representation Learning
#- Others <span>&#128065;&#65039;&#128065;&#65039; &#128071;&#128071;</span>
#- Reinforcement Learning
#- Unmanned Aerial Vehicles (UAV's)

# Education to show in About widget
education:
  courses:
  - course: PhD Information Science and Engineering (Expected)
    institution: Nara Institute of Science and Technology, Japan
    year: 2025
  - course: MEng Information Science and Engineering
    institution: Nara Institute of Science and Technology, Japan
    year: 2022
  - course: Research Student
    institution: Kyoto University, Japan
    year: 2020
  - course: BEng in Telecommunications Engineering
    institution: Kyambogo University, Uganda
    year: Jan 2016

# Social/Academic Networking
# For available icons, see: https://wowchemy.com/docs/getting-started/page-builder/#icons
#   For an email link, use "fas" icon pack, "envelope" icon, and a link in the
#   form "mailto:your-email@example.com" or "/#contact" for contact widget.
social:
- icon: envelope
  icon_pack: fas
  link: '/#contact'
- icon: twitter
  icon_pack: fab
  link: https://twitter.com/
- icon: graduation-cap  # Alternatively, use `google-scholar` icon from `ai` icon pack
  icon_pack: fas
  link: https://scholar.google.com/citations?view_op=list_works&hl=en&hl=en&user=2Li9kqwAAAAJ
- icon: github
  icon_pack: fab
  link: https://github.com/
- icon: linkedin
  icon_pack: fab
  link: https://www.linkedin.com/in/atuhurra-jesse-b2850ba8

# Link to a PDF of your resume/CV.
# To use: copy your resume to `static/uploads/resume.pdf`, enable `ai` icons in `params.toml`, 
# and uncomment the lines below.
- icon: cv
  icon_pack: ai
  link: uploads/resume.pdf

# Enter email to display Gravatar (if Gravatar enabled in Config)
email: "atuhurrajesse [at] gmail.com"

# Highlight the author in author lists? (true/false)
highlight_name: false
---

I am a PhD student at the <a href="https://nlp.naist.jp/en/">Natural Language Processing Lab</a> of NAIST, working with <a href="https://sites.google.com/site/tarowtnb/">Prof. Taro Watanabe</a>. I'm grateful for <a href="https://sites.google.com/site/hidetakakamigaito">Hidetaka Kamigaito</a>, <a href="https://hshindo.com/">Hiroyuki Shindo</a>, and <a href="https://hiroki13.github.io/">Hiroki Ouchi</a> guidance too.

My NLP research interests lie in Information Extraction (named entity recogntion, entity linking), Knowledge Graphs, Multimodal AI, prompting in Large Language Models (LLMs) and Low-resource NLP. Broadly speaking, I am passionate about applying deep learning approaches to enable machines to understand human language, and facilitate communication between humans and social robots.

I am currently working with <a href="https://www.dfki.de/en/web">Deutsches Forschungszentrum fÃ¼r KÃ¼nstliche Intelligenz GmbH (DFKI)</a> especially the <a href="https://www.dfki.de/en/web/about-us/locations-contact/berlin">DFKI Lab Berlin</a> to construct <i>language resources for Swahili and North European languages</i>. 
<!-- I'm working with <a href="https://dfki-nlp.github.io/authors/leonhard-hennig/">Leonhard Hennig</a>. -->

I worked on social robots under the <a href="https://grp.riken.jp/en/">Guardian Robot Project</a> at <a href="https://www.riken.jp/en/research/labs/r-ih/">RIKEN</a> where I am specifically contributed to <i>First-person Multimodal Perception</i> through <i>Attribute collection</i> and <i>Vision Language Models (VLMs)</i>. I worked with <a href="https://pomdp.net">Koichiro Yoshino</a>. 
<!-- RIKEN R-IH is different from RIKEN AIP -->

I undertook a research internship at <a href="https://www.fujitsu.com/global/about/research/">Fujitsu AI Lab</a> where I worked on <i>Multimodal Information Extraction</i>. I worked with <b>Prof. Tomoya Iwakura</b> and <a href="https://tathi.github.io/">Tatsuya Hiraoka</a>.

I was affiliated with <a href="http://www.jp.honda-ri.com/en/">HONDA Research Institute Japan (HRI-JP)</a> as a Part-time Researcher, and my work primarily focused on <i>Intent Recognition in Language</i> for a Social Robot. I collaborated with <a href="hhttps://scholar.google.co.jp/citations?user=I3_MfAMAAAAJ&hl=en">Eric Nichols</a> and <b>Anton de la Fuente</b>.

Previously, I worked on <i>Intrusion Detection</i> in IoT networks in the <a href="http://www-lsm.naist.jp/en/">Large-scale Systems Management Lab</a> of <a href="http://www.naist.jp/en/">NAIST</a> during my master's degree where <a href="http://www-lsm.naist.jp/~kasahara/index-e.html"> Prof. Shoji Kasahara</a> advised me.

I spent time as a Research Student at <a href="https://www.kyoto-u.ac.jp/en"> Kyoto University</a>, in <a href="https://www.db.soc.i.kyoto-u.ac.jp/doku.php/en:start"> Yoshikawa Lab </a> in the <a href="https://www.i.kyoto-u.ac.jp/en/"> Graduate School of Informatics </a>. While there, I was supervised and mentored by <a href="https://www.db.soc.i.kyoto-u.ac.jp/~yoshikawa/index-en.html"> Prof. Masatoshi Yoshikawa </a> on several methods mainly related to Information Retrieval, Databases, Human-Computer Interface design and Artificial Intelligence.

My graduate studies are fully funded by the Japanese government's <a href="https://www.mext.go.jp/en/policy/education/highered/title02/detail02/sdetail02/1373897.htm">MEXT</a> scholarship for which I am incredibly grateful.

<!--
<b style="color:green;">Research Manuscripts Under Preparation:</b> <br>
<p >1. Atuhurra Jesse, Takanori Hara, Yuanyu Zhang, and Shoji Kasahara, <b>OADIS: Online, Adaptive, Deep Learning based Intrusion Detection with SMOTE sampling in IoT networks</b>. </p>
<b style="color:green;">Research Manuscripts Under Review:</b> <br>
<p >Paper submitted to NAACL 2022. </p>
Keep this LINK: https://stackoverflow.com/questions/64468843/netlify-deployment-failed-during-stage-building-site-build-script-returned-n
keep another LINK: https://gohugo.io/hosting-and-deployment/hosting-on-netlify/
-->

<b style="color:red;">Activities: </b>
<ul>
  <li><b>[Jan. 2024]</b> Commenced work on multimodal perception for Robots, at RIKEN.</li>
  <li><b>[Sept. 2023]</b> Started research internship at Fujitsu AI Lab.</li>
  <li><b>[Feb. 2022]</b> Master Thesis Defense on <i>Balancing Data Distribution for Intrusion-detection Analysis</i>.</li>
  <li><b>[Oct. 2021]</b> Completed research internship, starting a new role at Honda.</li>
  <li><b>[Sept. 2021]</b> Selected to participate in the AllenNLP Hacks 2021.</li>
</ul>

<h6><b style='color:purple !important;'>Published (Robotics): </b></h6>
<ul>
    <li>Zero-shot Retrieval of User Intent in Human-Robot Interaction with Large Language Models, IEEE International Conference on Multimedia Information Processing and Retrieval (IEEE MIPR 2024), San Jose, CA, USA (August 7-9, 2024).</li>
    <li>The Impact of Large Language Models on Social Robots: Potential Benefits and Challenges, Assistive Robots @ Robotics: Science and Systems (RSS 2024), Delft, Netherlands (July 15 - 19, 2024).</li>
</ul>

<!--
<h3><b style='color:purple !important;'>Google Scholar ðŸ‘‰ &#128073; </b> <a href="https://scholar.google.com/citations?view_op=list_works&hl=en&hl=en&user=2Li9kqwAAAAJ">Here.</a></h3>
-->

<!-- {{< icon name="download" pack="fas" >}} Download my {{< staticref "uploads/demo_resume.pdf" "newtab" >}}resumÃ©{{< /staticref >}}. -->

<!-- The stufff below works -->

<!-- 
<script type="text/javascript" src="//rf.revolvermaps.com/0/0/6.js?i=5ird9uhh872&amp;m=7&amp;c=e63100&amp;cr1=ffffff&amp;f=arial&amp;l=0&amp;bv=90&amp;lx=-420&amp;ly=420&amp;hi=20&amp;he=7&amp;hc=a8ddff&amp;rs=80" async="async"></script> 
-->
